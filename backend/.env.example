# ==========================================
# 知识库后端环境配置
# ==========================================

# ------------------------------------------
# 数据库配置
# ------------------------------------------

# 是否使用嵌入式 PostgreSQL（便携版）
# true = 使用 embedded postgres（推荐用于桌面应用）
# false = 使用外部 PostgreSQL 服务器
USE_EMBEDDED_PG=true

# 数据库配置（嵌入式和外部模式共用）
# 嵌入式模式默认值: localhost:15432, 外部模式请修改为实际地址
DATABASE_HOST=localhost
DATABASE_PORT=15432              # 嵌入式使用非标准端口避免冲突，外部数据库通常用 5432
DATABASE_USER=postgres
DATABASE_PASSWORD=postgres
DATABASE_NAME=knowledge_base

# ------------------------------------------
# OpenAI API 配置
# ------------------------------------------
OPENAI_API_KEY=your_openai_api_key_here

# 可选：如果使用 Azure OpenAI
# OPENAI_API_BASE=https://your-resource.openai.azure.com/
# OPENAI_API_VERSION=2024-02-01

# ------------------------------------------
# 服务配置
# ------------------------------------------
APP_VERSION=0.1.0          # 应用版本号
DEBUG=true                 # 调试模式
PORT=8000                 # 服务端口

# 以下为各模块数据目录配置，支持相对路径（相对于 backend/）或绝对路径
# 打包后建议改为绝对路径或相对于可执行文件的路径
POSTGRES_DIR=postgres                # PostgreSQL 数据目录，默认: backend/postgres/
STORE_DIR=store                      # 文件存储目录，默认: backend/store/
MODELS_DIR=models                    # AI 模型目录，默认: backend/models/
TEMP_DIR=temp                        # 临时文件目录，默认: backend/temp/

# ------------------------------------------
# 日志配置
# ------------------------------------------
LOG_LEVEL=INFO            # 日志级别: DEBUG, INFO, WARNING, ERROR

# ------------------------------------------
# MinerU PDF 解析配置
# ------------------------------------------

# 解析后端 - 控制 PDF 解析使用的引擎
# 可选值:
#   pipeline           - 通用解析模式（默认），速度快，资源占用少，推荐
#   vlm-auto-engine    - 高精度本地推理（需要额外下载 ~10GB 模型）
#                        - Mac (Apple Silicon): 自动使用 MLX 引擎
#                        - Windows: 自动使用 lmdeploy 或 transformers
#                        - Linux: 自动使用 vllm、lmdeploy 或 transformers
#   hybrid-auto-engine - 下一代高精度本地推理，质量更高但速度较慢
#   vlm-http-client    - 远程推理，需要配合 OpenAI 兼容服务器
#   hybrid-http-client - 混合远程推理
#
# 注意:
#   - vlm-auto-engine 在 Apple Silicon Mac 上会自动使用 MLX 加速
#   - 不支持 vlm-mlx-engine，请使用 vlm-auto-engine
#   - 需要安装额外的依赖: pip install mineru[vlm] 或 pip install mlx-vlm (Mac)
#   - VLM 模式需要下载额外的模型文件 (~10GB)，如果模型缺失会报错
#   - pipeline 后端和 VLM 后端是两种不同的解析方式
#
# 如果遇到 "pytorch_model.bin not found" 错误，说明 VLM 模型未下载，请：
#   1. 使用 pipeline 模式（推荐）: MINERU_BACKEND=pipeline
#   2. 或下载完整模型: mineru download-models
MINERU_BACKEND=pipeline

# 设备模式 - 仅当 backend=pipeline 时生效
# 控制 pipeline 模式的硬件加速
# 可选值:
#   cpu          - 使用 CPU (默认，兼容性最好，所有平台支持)
#   cuda         - 使用 NVIDIA GPU CUDA 加速
#   cuda:0       - 使用指定 NVIDIA GPU
#   mps          - 使用 Apple Silicon GPU (Metal Performance Shaders)
#   npu          - 使用华为 NPU 加速
#   npu:0        - 使用指定华为 NPU
MINERU_DEVICE=cpu

# GPU 显存限制 (MB) - 仅当 backend=pipeline 且使用 GPU 设备时生效
# 设置单进程 GPU 显存上限，防止内存溢出
# 例如: 4096 = 4GB, 8192 = 8GB
# MINERU_VRAM=4096

# VLM 模式虚拟显存大小 (GB) - 仅当 backend=vlm-* 或 hybrid-* 时生效
# 用于覆盖自动检测的显存大小，影响 batch_size 的自动计算
# 如果你的 GPU 显存检测不正确（如 Apple Silicon 显示为 1GB），请设置此值
# 例如: 8 = 8GB, 16 = 16GB, 24 = 24GB
# MINERU_VIRTUAL_VRAM_SIZE=8

# VLM 模式批处理大小
# 注意: MinerU 通过 MINERU_VIRTUAL_VRAM_SIZE 自动计算 batch_size:
#   显存 >= 16GB -> batch_size=8
#   显存 >= 8GB  -> batch_size=4
#   显存 < 8GB   -> batch_size=1
